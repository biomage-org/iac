name: Deploy cluster monitoring

on:
  workflow_dispatch:
    inputs:
      environment_name:
        type: choice
        description: Select the environment name to run the actions on
        options:
          - trv
          - Biomage
          - all
        default: all
      environment_type:
        type: choice
        description: Select environment type
        options:
          - staging
          - production
          - staging and production
        default: staging

# this ensures that only one CI pipeline with the same key
#  can run at once in order to prevent undefined states
concurrency: cluster-update-mutex

jobs:
  set-environments:
    name: Set up environment name and environment type for action run
    runs-on: ubuntu-20.04
    outputs:
      env-type: ${{ steps.set-env-type.outputs.env-type }}
      env-name: ${{ steps.set-env-name.outputs.env-name }}
    steps:
      - id: set-env-type
        name: Set up environment type
        run: |-
          if [ "${ENVIRONMENT_TYPE}" = "staging" ]; then
            echo 'env-type=["staging"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_TYPE}" = "production" ]; then
            echo 'env-type=["production"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_TYPE}" = "staging and production" ]; then
            echo 'env-type=["staging", "production"]' >> $GITHUB_OUTPUT
          fi
        env:
          ENVIRONMENT_TYPE: ${{ github.event.inputs.environment_type }}
      - id: set-env-name
        name: Set up environment name
        run: |-
          if [ "${ENVIRONMENT_NAME}" = "all" ]; then
            echo 'env-name=["Biomage", "trv"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "trv" ]; then
            echo 'env-name=["trv"]' >> $GITHUB_OUTPUT
          elif [ "${ENVIRONMENT_NAME}" = "Biomage" ]; then
            echo 'env-name=["Biomage"]' >> $GITHUB_OUTPUT
          fi
        env:
          ENVIRONMENT_NAME: ${{ github.event.inputs.environment_name }}

  check-secrets:
    name: Check that sufficient secrets are specified for environment name
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
    environment: ${{ matrix.environment-name }}
    steps:
      - id: check-secrets
        name: Check if necessary secrets are installed.
        run: |-
          echo Checking if required secrets are defined in the repository.

          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ]
          then
            echo AWS Access Key ID not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]
          then
            echo AWS Secret Access Key not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.API_TOKEN_GITHUB }}" ]
          then
            echo GitHub deploy key access token not defined.
            ERROR=true
          fi
          if [ -z "${{ secrets.DATADOG_API_KEY}}" ]
          then
            echo Datadog API key is not defined.
            ERROR=true
          fi
          if [ ! -z "$ERROR" ]
          then
            echo
            echo This workflow requires some secrets to complete.
            echo Please make sure they are created by adding/rotating them manually.
            exit 1
          fi

  deploy-monitoring:
    name: Setup logging and monitoring
    runs-on: ubuntu-20.04
    needs: check-secrets
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
      API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
    strategy:
      matrix:
        environment-type: ${{ fromJson(needs.set-environments.outputs.env-type) }}
        environment-name: ${{ fromJson(needs.set-environments.outputs.env-name) }}
        exclude:
          - environment-name: trv
            environment-type: staging
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - id: add-kubeconfig
        name: Add k8s config file for existing cluster.
        run: |-
          aws eks update-kubeconfig --name biomage-$CLUSTER_ENV

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: setup-cluster-cloudwatch-logging-policy
        name: Setup permissions required for cluster to log to Cloudwatch
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }}"
          name: "cluster-cloudwatch-logging-policy-${{ matrix.environment-type }}"
          template: 'infra/cluster-logging/cf-cluster-log-cloudwatch-policy.yaml'
          no-fail-on-empty-changeset: "1"
          capabilities: "CAPABILITY_IAM,CAPABILITY_NAMED_IAM"

      # Setting up log forwarding for pods hosted in EC2 nodes
      - id: create-fluent-bit-namespace
        name: Create namespace for node FluentBit deployment
        run: kubectl apply -f infra/cluster-logging/node-fluentbit-namespace.yaml

      - id: create-service-account-for-node-fluent-bit
        name: Create service account for node FluentBit
        env:
          LOGGING_POLICY_ARN: ${{ steps.setup-cluster-cloudwatch-logging-policy.outputs.PolicyARN }}
        run: |-
          eksctl create iamserviceaccount \
            --name fluent-bit \
            --namespace node-logging \
            --cluster biomage-$CLUSTER_ENV \
            --role-name irsa-fluent-bit-$CLUSTER_ENV \
            --attach-policy-arn $LOGGING_POLICY_ARN \
            --override-existing-serviceaccounts \
            --approve

      - id: deploy-node-fluent-bit
        name: Deploy FluentBit for EC2 nodes
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          # FluentBit configuration is determined in infra/cluster-logging/node-fluentbit-config.yaml, specifically under [INPUT] > Path
          # We do not want to log everything for costs/security concerns

          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_CLUSTER_ENV\", \"$CLUSTER_ENV\")" infra/cluster-logging/node-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_AWS_REGION\", \"$AWS_REGION\")" infra/cluster-logging/node-fluentbit-config.yaml

          kubectl apply -f infra/cluster-logging/node-fluentbit-config.yaml

      # Setting up log forwarding for pods hosted on Fargate nodes
      - id: attach-pod-execution-role-name
        name: Attach logging policy to pod execution role
        env:
          LOGGING_POLICY_ARN: ${{ steps.setup-cluster-cloudwatch-logging-policy.outputs.PolicyARN }}
        run: |-
          # Pods launched in the same cluster has the same pod execution role, as pod execution role scope is cluster-wide.
          # See https://eksctl.io/usage/fargate-support/#creating-a-cluster-with-fargate-support
          # Getting fargate-profile of pipeline or worker in the same cluster gets the same pod execution role.

          POD_EXEC_ROLE_NAME=$(aws eks describe-fargate-profile \
            --cluster-name biomage-$CLUSTER_ENV \
            --fargate-profile-name pipeline-default | jq -r '.fargateProfile.podExecutionRoleArn' | awk -F"/" '{print (NF>1)? $NF : ""}' )

          aws iam attach-role-policy --role-name $POD_EXEC_ROLE_NAME --policy-arn $LOGGING_POLICY_ARN

      - id: deploy-fargate-fluent-bit
        name: Deploy FluentBit config for Fargate pods
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |-
          # FluentBit configuration is determined in infra/cluster-logging/fargate-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_CLUSTER_ENV\", \"$CLUSTER_ENV\")" infra/cluster-logging/fargate-fluentbit-config.yaml
          yq -i "(.. | select(type == \"!!str\")) |= sub(\"CI_AWS_REGION\", \"$AWS_REGION\")" infra/cluster-logging/fargate-fluentbit-config.yaml

          kubectl apply -f infra/cluster-logging/fargate-fluentbit-config.yaml

      # Setting up Datadog to watch pod metrics for pods hosted on EC2 and Fargate nodes
      - id: setup-datadog-cluster-agent
        name: Setup Datadog cluster agent
        run: |-
          helm repo add datadog https://helm.datadoghq.com
          helm repo update
          helm upgrade datadog-agent datadog/datadog \
            -f infra/datadog/cluster-agent-values.yaml \
            --set datadog.apiKey=${{ secrets.DATADOG_API_KEY }} \
            --set datadog.clusterName=biomage-$CLUSTER_ENV \
            --install

      - id: setup-datadog-sidecar-permissions
        name: Setup Datadog sidecar permissions
        run: |-
          kubectl apply -f infra/datadog/datadog-sidecar-rbac.yaml

  report-if-failed:
    name: Report if workflow failed
    runs-on: ubuntu-20.04
    needs: [check-secrets, deploy-monitoring]
    if: failure() && github.ref == 'refs/heads/master'
    steps:
      - id: send-to-slack
        name: Send failure notification to Slack on failure
        env:
          SLACK_BOT_TOKEN: ${{ secrets.WORKFLOW_STATUS_BOT_TOKEN }}
        uses: voxmedia/github-action-slack-notify-build@v1
        with:
          channel: workflow-failures
          status: FAILED
          color: danger
